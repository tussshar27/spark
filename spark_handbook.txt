Spark Architecture:

1. driver: acts as an architect. has the blueprint (application code) and co-ordinates all the work.
it is the heart of the spark application.
it manages information and state of executors.
analyzes, distributes and schedules the work on executors.

2. cluster manager: it is the resource manager.

3. executors: are the workers on the site. each executor is a JVM container running on a different machine.
it execute the code.
report the status of execution to the driver.

together forms master-slave architecture.

Points:
1. each task can only work on one partition of data at a time.
2. executors host cores and each core can run 1 task at a time.
3. tasks can execute in parallel.
4. to allow every executors to work in parallel, spark breaks down the data into chunks called partitions.

working:
1. user assign job to driver.
2. driver internally analyzes , distributes and breaks down job into stages and tasks and assign it to executors.
3. executors consist JVM containers and cores. 

Apache spark:
it is an open source, distributed data processing engine designed for speed and ease to use.
features:
1. (RAM) in-memory processing, allows it to run 100x faster than Hadoop mapreduce having (Disk) processing.

Flow of spark application run:
below is the step-by-step flow when you submit a spark job.
1. Submission: user submits spark application (JAR/python file) to the cluster.
2. Driver starts: the cluster manager starts the driver program on one of the node in the cluster.
3. SparkContext Registration: the driver starts the SparkContext.
4. Resource Acquisition: SparkContext asks the cluster manager for resources (Executors).
5. Executor Launch: the cluster manager launches executors on the worker nodes and report their network addresses back to the driver.
6. Task Assignment: the driver now talks directly to the executors. it sends the application code and tasks to the executors.
7. Execution: each executor runs the tasks on its portion of the data, performing transformations and actions.
8. Result collection: the executors return the results or the status of the tasks back to the driver.
9. Shutdown: once the job is complete, the SparkContext stops, and the cluster manager shuts down the executors, freeing up resources for other applications.

Diagram:
[Client Machine]
     |
     ‚Üì (submit job)
[Driver Program] ‚Üê---‚Üí [Cluster Manager]
     |                      |
     ‚Üì (coordinates)        ‚Üì (allocates)
    [Executors on Worker Nodes]
     |
     ‚Üì (parallel processing)
[Task][Task][Task]... across cluster

1. Partition: To allow every executor to work in parallel, spark breaks down the data into chunks called partitions.

2. Transformation: The instructions or code that we write in order to modify/transform our data.
Eg. select, where, groupBy, etc.
Two types:
a. Narrow Transformation: (one to one) one partition contribute transformation to one partition
b. Wide Transformation: (one to many) one partition contribute transformation to multiple partitions. lead to data shuffle.
Transformation help in building the logical plan.
Logical plan:

         Emp
          ‚îÇ
          ‚ñº
       +--------+
       |  Data  |
       +--------+
          ‚îÇ
  -- Transformation: where Sal > 160000
          ‚îÇ
          ‚ñº
       +--------+
       |  Data  |
       +--------+
          ‚îÇ
 -- Transformation: select name, dept_id, salary
          ‚îÇ
          ‚ñº
       +--------+
       |  Data  |
       +--------+
          ‚îÇ
    -- Transformation: group by dept_id
          ‚îÇ
          ‚ñº
     +--------------+
     |  Final Data  |
     +--------------+

3. Action: In order to execute the logical plan what transformation creates, action is created.
Action triggers the execution plan.
Three types:
a. View data
b. Collect data
c. Write data

when an action is called:
Eg. .show()
a. Spark analyzes the logical plan
b. Optimizes it into an execution plan
c. Executes the plan on the cluster

Computation Graph: It is the logical plan created by all transformations.

4. Lazy Evaluation: Spark does not execute any transformations until an action is called.
so spark will wait for action trigger in order to execute the computation graph.
But why spark does that?
Eg. Sandwitch analogy: A customer asked for a white bread sandwitch and the chef started making it. later customer changes its requirement to have a brown bread sandwitch. due to this the chef time got wasted and the white bread and other ingredients also got wasted. to avoid this wastage in future, chef decided to take money first (Action) then start preparation of food. so here money becomes action and food making becomes transformations.
Due to similar scenario, spark waits for all transformations code so that to form chronology of optimized execution plan using Catalyst optimizer and perform all the transformations once action is triggered.

5. Spark Session: Spark session is the entry point of spark execution. The driver process is known as spark session.

What is an API?
API is a bridge that allows to services to communicate with each other without knowing how the data is prepared in its origin
Eg. just like how a waiter takes your order from you and brings back your food from the kitchen. And here you don't need to know how the kitchen makes the food.
An API defines how software components should interact.
It exposes specific endpoints (URLs) and methods that other systems can call.
here in spark there High level APIs (Dataframe, Dataset, SQL) and Low Level APIs (RDD)
Spark Eg.:
df = spark.read.csv("path/to/data.csv", header=True)
df.filter(df["age"] > 30).select("name", "age").show()
- spark.read.csv() ‚Üí part of the DataFrameReader API
- filter() and select() ‚Üí part of the DataFrame API
- show() ‚Üí part of the action API

what is a Dataframe?
"A DataFrame in Apache Spark is a distributed collection of data organized into named columns, similar to a table in a relational database."
Dataframe is the structured API, represented like a table.
table is formed by rows and columns.
Dataframe has schemas, which is the metadata for the columns.
Data is stored in partitions.
it is immutable means it cannot be modified, new dataframe can be created out of that dataframe.
Diagram:
                         +-------------------+
                         |   DataFrame 1     |
                         |   (emp_data)      |
                         +-------------------+
                                   ‚îÇ
                                   ‚îÇ where sal > 100000
                                   ‚ñº
                         +-------------------+
                         |   DataFrame 2     |
                         +-------------------+
                                   ‚îÇ
                                   ‚îÇ select (name, dept_id, sal)
                                   ‚ñº
                         +-------------------+
                         |   DataFrame 3     |
                         +-------------------+
                    

Notes:
- **Immutability:** In Spark, DataFrames are immutable. Once created, they cannot be changed.
- **Cascade of Transformations:** Each transformation (e.g., `where`, `select`) returns a new DataFrame.
- **Lazy Evaluation:** These transformations only build a *logical plan*. Actual computation happens when an *action* (like `show()`, `collect()`, or `write()`) is triggered.
- **Optimization:** The Catalyst optimizer combines and optimizes all transformations before execution.

spark internally does this:
User Code
   ‚îÇ
   ‚ñº
Unresolved Logical Plan  ‚Üê Columns, tables not yet verified
   ‚îÇ
   ‚ñº
Analyzed Logical Plan    ‚Üê Validated schema, column names, etc.
   ‚îÇ
   ‚ñº
Optimized Logical Plan   ‚Üê Catalyst optimizer applies rules
   ‚îÇ
   ‚ñº
Physical Plan            ‚Üê Actual execution strategy
   ‚îÇ
   ‚ñº
Execution on cluster     ‚Üê RDD tasks sent to executors

what is DAG?
DAG stands for Directed Acyclic Graph.
It‚Äôs a graph-based representation of all the transformations (like select, filter, groupBy, etc.) that Spark needs to perform on your data.
In simple terms:
üëâ A DAG is the execution plan Spark builds before running your job.

Practical:
#creating spark session object
from pyspark.sql import SparkSession          #importing SparkSession class

spark = (
     SparkSession
     .builder                         # using builder method from SparkSession class
     .appName("Spark Inroduction")     #giving name to our spark session object
     .master("local[*]")     #location where the code will be executed
     .getOrCreate()
)

#For every spark object that we create, spark provided its application spark UI in output.
localhost:4040





