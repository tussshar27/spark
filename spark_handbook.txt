Spark Architecture:

1. driver: acts as an architect. has the blueprint (application code) and co-ordinates all the work.
it is the heart of the spark application.
it manages information and state of executors.
analyzes, distributes and schedules the work on executors.

2. cluster manager: it is the resource manager.

3. executors: are the workers on the site. each executor is a JVM container running on a different machine.
it execute the code.
report the status of execution to the driver.

together forms master-slave architecture.

Points:
1. each task can only work on one partition of data at a time.
2. executors host cores and each core can run 1 task at a time.
3. tasks can execute in parallel.
4. to allow every executors to work in parallel, spark breaks down the data into chunks called partitions.

working:
1. user assign job to driver.
2. driver internally analyzes , distributes and breaks down job into stages and tasks and assign it to executors.
3. executors consist JVM containers and cores. 

Apache spark:
it is an open source, distributed data processing engine designed for speed and ease to use.
features:
1. (RAM) in-memory processing, allows it to run 100x faster than Hadoop mapreduce having (Disk) processing.

Flow of spark application run:
below is the step-by-step flow when you submit a spark job.
1. Submission: user submits spark application (JAR/python file) to the cluster.
2. Driver starts: the cluster manager starts the driver program on one of the node in the cluster.
3. SparkContext Registration: the driver starts the SparkContext.
4. Resource Acquisition: SparkContext asks the cluster manager for resources (Executors).
5. Executor Launch: the cluster manager launches executors on the worker nodes and report their network addresses back to the driver.
6. Task Assignment: the driver now talks directly to the executors. it sends the application code and tasks to the executors.
7. Execution: each executor runs the tasks on its portion of the data, performing transformations and actions.
8. Result collection: the executors return the results or the status of the tasks back to the driver.
9. Shutdown: once the job is complete, the SparkContext stops, and the cluster manager shuts down the executors, freeing up resources for other applications.

Diagram:
[Client Machine]
     |
     ↓ (submit job)
[Driver Program] ←---→ [Cluster Manager]
     |                      |
     ↓ (coordinates)        ↓ (allocates)
    [Executors on Worker Nodes]
     |
     ↓ (parallel processing)
[Task][Task][Task]... across cluster







