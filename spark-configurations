To process 25GB of data in spark:
a. How many CPU cores are required to process 25GB of data?
By default, spark creates one partition for each block of the file (block is of 128mb in HDFS).
Converting value in GB to MB:
25GB = 25 * 1024 MB = 25600 MB.

Total number of partitions = total size of data/ size of each partition = 25600/128 = 200

Number of cores = Number of Partitions.
Therefore, 200 CPU cores are required to process 25 GB data in PARALLEL.

b. How many executors are required to process 25 GB?
As per the researchers, 2 to 5 is the range of cores for each executor.
We have to do R&D like how is the job behaving, to find the suitable CPU cores for our executor by performing using 2 cores, 3 cores, etc.
Average CPU cores per executor: 4

Total number of executors = total CPU cores/ Avg CPU cores per executor = 200/4 = 50

c. How much each executor memory is required to process 25 GB of data?
Expected RAM memory for each core = minimum 4 times of * (default partition size) = 4*128 MB = 512 MB RAM per core.
Executor RAM memory should not be less than 1.5 * of spark reserved memory.(single core executor memory should not be les than 450 MB)
RAM Memory for each executor = CPU cores for each executor * RAM memory for each core = 4 * 512 MB = 2 GB
That means if we give 2 GB memory then only it will process 4 task in parallel.
Suppose, if we give 1 GB instead of 2 GB then we have to reduce the number of cores or the job wil get failed.

d. What is total memory required to process 25 GB data?
Total number of executor = 50
Memeory for each executor = 2 GB
Total memory for all executor = 50*2 GB = 100 GB.

Thumb rule:
Data = 25 GB
No. of CPU cores = 4 * data in GB = 4 * 25 = 200 cores.
Total No. of executors = 2 times of Data = 2 * 25 GB = 50 GB.
Memory for each executor = 4 * 512 MB = 2 GB.

Scenario Questions:
1. Suppose we have data of 25 GB to process. If one task is taking 5mins to complete then how much time will it take to complete all tasks?
For 25 GB of data, we have 200 cores which perform in parallel. so it will take 5mins only for all the 200 cores/tasks to complete the job.

2. Suppose client is assigning only 25 executors instead of 50 executors then how much time will it take?
Since it is taking 5 mins for 50 executors, it would take 10 mins for 25 executors.
and the total memory will required: 50 GB. Since for 50 executors, 100 GB of memory was required.
So, out of 200 only 100 tasks/cores will perform at a time in parallel and the next 100 tasks/cores will wait in a queue for 5 mins to complete it. So total it will take 10 mins to perform all the tasks.

NOTE: Suppose there is no performance issue for client that means the client is fine if the job takes 10 mins to complete instead of 5 mins then use 25 executors instead of 50 executors.



-------------------------------------------------------------------------------------------------------



Process 100GB/1TB of data.

1. how many executors are required?
2. how many cores are required?
3. what will be the driver size?
4. what will be the driver memory?

real time example:
we have 16 machines: 2 master(1 standby master) and 14 worker machines.

all master and workers machines are connected to each other.
master knows each and every worker machine configuration like IP, memory, etc.

RAM: 128GB per machine
Total RAM: 128GB * 14 workers = 1792GB = 1.8TB

CORES: 48 CPU per machine
Total CORES: 48 * 14 = 672 CPU CORES

These total RAM and CORES are divided into 5 teams in a firm.
And each team is running multiple spark applications having different size as shown below:
Team1 = app1, app2, app3
Team2 = app1, app2, app3, app4, app5, app6, app7
Team3 = app1, app2
Team4 = app1
Team5 = app1, app2, app3, app4, app5
Total = 18 spark applications













