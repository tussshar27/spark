To process 25GB of data in spark:
a. How many CPU cores are required to process 25GB of data?
By default, spark creates one partition for each block of the file (block is of 128mb in HDFS).
Converting value in GB to MB:
25GB = 25 * 1024 MB = 25600 MB.

Total number of partitions = total size of data/ size of each partition = 25600/128 = 200

Number of cores = Number of Partitions.
Therefore, 200 CPU cores are required to process 25 GB data in PARALLEL.

b. How many executors are required to process 25 GB?
As per the researchers, 2 to 5 is the range of cores for each executor.
We have to do R&D like how is the job behaving, to find the suitable CPU cores for our executor by performing using 2 cores, 3 cores, etc.
Average CPU cores per executor: 4

Total number of executors = total CPU cores/ Avg CPU cores per executor = 200/4 = 50

c. How much each executor memory is required to process 25 GB of data?
Expected RAM memory for each core = minimum 4 times of * (default partition size) = 4*128 MB = 512 MB RAM per core.
Executor RAM memory should not be less than 1.5 * of spark reserved memory.(single core executor memory should not be les than 450 MB)
RAM Memory for each executor = CPU cores for each executor * RAM memory for each core = 4 * 512 MB = 2 GB
That means if we give 2 GB memory then only it will process 4 task in parallel.
Suppose, if we give 1 GB instead of 2 GB then we have to reduce the number of cores or the job wil get failed.

d. What is total RAM memory required to process 25 GB data?
Total number of executor = 50
RAM Memeory for each executor = 2 GB
Total RAM memory for all executor = 50*2 GB = 100 GB.

Thumb rule:
Data = 25 GB
No. of CPU cores = 4 * data in GB = 4 * 25 = 200 cores.
Total No. of executors = 2 times of Data = 2 * 25 GB = 50 GB.
Memory for each executor = 4 * 512 MB = 2 GB.

Scenario Questions:
1. Suppose we have data of 25 GB to process. If one task is taking 5mins to complete then how much time will it take to complete all tasks?
For 25 GB of data, we have 200 cores which perform in parallel. so it will take 5mins only for all the 200 cores/tasks to complete the job.

2. Suppose client is assigning only 25 executors instead of 50 executors then how much time will it take?
Since it is taking 5 mins for 50 executors, it would take 10 mins for 25 executors.
and the total memory will required: 50 GB. Since for 50 executors, 100 GB of memory was required.
So, out of 200 only 100 tasks/cores will perform at a time in parallel and the next 100 tasks/cores will wait in a queue for 5 mins to complete it. So total it will take 10 mins to perform all the tasks.

NOTE: Suppose there is no performance issue for client that means the client is fine if the job takes 10 mins to complete instead of 5 mins then use 25 executors instead of 50 executors.



-------------------------------------------------------------------------------------------------------



Process 100GB/1TB of data.

1. how many executors are required?
2. how many cores are required?
3. what will be the driver size?
4. what will be the driver memory?

real time example:
we have 16 machines: 2 master(1 standby master) and 14 worker machines.

all master and workers machines are connected to each other.
master knows each and every worker machine configuration like IP, memory, etc.

RAM: 128GB per machine
Total RAM: 128GB * 14 workers = 1792GB = 1.8TB

CORES: 48 CPU per machine
Total CORES: 48 * 14 = 672 CPU CORES

These total RAM and CORES are divided into 5 teams in a firm.
And each team is running multiple spark applications having different size as shown below:
Team1 = app1, app2, app3
Team2 = app1, app2, app3, app4, app5, app6, app7
Team3 = app1, app2
Team4 = app1
Team5 = app1, app2, app3, app4, app5
Total = 18 spark applications

Reverse Calculation:
1 CPU core and 1GB RAM is reserved for the worker node to send a heartbeat.
So,
Total RAM: (128GB-1GB) * 14 workers = 1778GB ~= 1.8TB
Total CORES: (48-1) * 14 ~= 650 CPU CORES

Assume,
5 cores per executor  = 650/5 = 130 executors.
Average RAM memory per executor = Total RAM/Total Executors = 1.8TB/130 = 14GB
2nd eg for better understanding above calculation, 100GB TOTAL RAM/2 executors = 50GB per executor.

overhead memory exclusion: 10% of total memory or 384MB which ever is lower.
=14GB - 14GB*10% = 14GB - 1.4GB  = 12.5GB per executor for 5 core per executor.
Now, this solution is good for ideal scenario as shown below
Team1 = app1, app2, app3
Team2 = app1, app2, app3 
Team3 = app1, app2, app3
Team4 = app1, app2, app3
Team5 = app1, app2, app3
but for real world scenario it won't work since each team is working on different spark application with different configuration size.

Real Calculation:
Ask the questions mentioned below first:
1) Requirement gathering for 100GB of data-
1. Type of data: csv, parquet, json, XML, etc
2. Type of transformation: multiple joins or joining with 10GB of external data or wide transformations or dumping the data using select statement, etc
3. Single vs multiple sources: suppose 10GB comes from DB and 90GB comes from file system.
4. Any SLA (Service Level Agreement): time should be taken by spark job to complete, eg: 30mins, 1hr, 2hr, etc

2) Cluster configuration-
what is your cluster configuration?

3)Your approach/mindset-
Calculate the partitions:
main data = 100GB = 800 partitions = 125mb partition size
external data join during transformation data = 10GB = 80 partitions = 125mb partition size
then, total data 110GB.

Approach1: 
lets say we have more than 50GB data then take its half as RAM which is 25GB and check whether the SLA is met or not, it should be done in trial and error basis.
if data is of 10GB then RAM 20GB, 1GB then RAM 4GB+1GB additional memory.
so if the data is big then we divide it and if the data is small then we double it.

for 110gb data, considering 60gb RAM = 15GB RAM per executor, 5 cores per executor, 4 executors. so 15*4=60GB.
60GB RAM is divided in three memories:
300mb * 4machines = 1.2gb of reserve memory
60% of 59gb = 36gb of storage executor memory
40% of 59gb = 24gb of user memory

optimization is done in above storage executor memory and user memory as shown below we have to do trial and error to check SLA.
can we run it in 70%-30% configuration?
can we run it in 50%-50% configuration?
















