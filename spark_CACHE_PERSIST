https://youtu.be/5peB6KcCsd4?si=silODGnJri3ySpq0

from pyspark.sql import SparkSession
spark = (
  SparkSession
  .builder
  .appName("Cache and Persist")
  .master("local[*]")                              #using local machine instead of spark standalone cluster
  .config("spark.executor.memory","512M")          #restricting executor memory to 512MB
  .getOrCreate()
)

spark

now go to spark UI:
www.localhost:4040/
go to Environment tab > verify the memory set for the executor
spark.executor.memory = 512M
spark.master = local[*]

#read sales csv file of 752mb in size ~ 7.2M records
_schema = "transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string"
df = spark.read.format("csv").schema(_schema).option("header",True).load("data/input/file1")

#since we are already defining the schema, there will be no job created after running above code

df.where("amount > 300").show()
#after running above where clause, go to spark UI > SQL/Dataframe tab > open the job created > we can see scan the file DAG
#so we have already scanned our file in first step but here while using where clause also spark is again scanning the file.

#lets cache the dataframe
df.cache()
#after running above code, there is no job or storage RDD created in spark UI
#in order to actually cache the dataframe, we need to run action
#because cache() need action, and count and write are preferred since they scan the whole dataset resulting in proper caching.

df.cache().count()
#now go to spark UI > storage tab > storage RDD is created for cache
#notice the storage level column, it is "Disk Memory Deserialized 1x Replicated" which is default setting of cache
#check "size in memory" and "size on disk" columns, and some of the data is in memory and some of the data spilled to disk.
#because we have a dataset which is greater than the memory of the executor.

#IMPORTANT
#NOTE: the default storage level for cache in dataframe and dataset is MEMORY_AND_DISK .
#NOTE: the default storage level for cache in RDD is MEMORY_ONLY.

df.where("amount > 300").show()
#go to spark UI > SQL/Dataframe, we can see a job is created, open it > scan csv = 0, InMemoryTableScan = 10000 records scanned.
#csv is 0 because this execution does not hit the csv rather it has InMemoryTableScan which is the cache that we just created.
#so cache is created in memory and disk.

#to remove the cache
df.unpersist()
now go to spark UI > storage tab, we can see cache is removed.

#even if run unpersist, and if we have created multiple inherited dataframe, there might be cache in spark UI still showing even afger running unpersist(). so to remove all the existing cache, run below command.
spark.catalog.clearCache()

df_cache = df.cache()

df_cache.count()

df.where("amount > 100").show()

#in the above three lines, we are creating cache but we are filtering data on the parent dataframe then also spark will use cache dataframe in order to scan the data. 
#that's why we should be aware while caching filtered data as well otherwise it will do partial scanning.

#for example:
df_cache = df.filter("amount > 100").cache()    
df_cache.count()      #since we are caching filtered data which is less than the executor memory size. so when you go to spark UI > storage tab, we can see it has utilized only memory excluding disk in "size in memory" column.
df.filter("amount > 50").show()
#so if we apply different filter then we spark will read it from csv.
#now it will do a partial scan and if we open the job in spark UI, we can see it has also read the csv file.

#doing cache with different storage level setting, use persist method

# MEMORY_ONLY , MEMORY_AND_DISK , MEMORY_ONLY_SER , MEMORY_AND_DISK_SER , DISK_ONLY , MEMORY_ONLY_2 , MEMORY_AND_DISK_2

#MEMORY_ONLY_SER  : used for scala/java job
#MEMORY_ONLY_SER / MEMORY_AND_DISK_SER  : in pyspark, they are not used because by default it store serialized data in memory.
#MEMORY_ONLY_2 / MEMORY_AND_DISK_2  : go to spark UI after running > storage tab:  we get 2x Replicated data in all the executors.

#IMPORTANT:
#The Storage tab in the Spark UI shows information about cached / persisted DataFrames or RDDs.

import pyspark
df_persist = df.persist(pyspark.StorageLevel.MEMORY_ONLY)

#this time, instead of count we will do no operations write
df.persist.write.format("noop").mode("overwrite").save()

#now go to spark UI > storage tab: we can see RDD with "Memory Serialized 1x Replicated". 
#we can see it has used Memory in serialized because cache puts data in unserialized and persist puts in serialized format.
#when the size of data is more than the memeory than at that time we will see OOM Out of Memory error. so be very sure when to use MEMORY_ONLY storage level setting.

| Storage Level         | Serialization | Notes                        |
| --------------------- | ------------- | ---------------------------- |
| `MEMORY_ONLY`         | ‚ùå No          | Fastest, deserialized        |
| `MEMORY_AND_DISK`     | ‚ùå No          | Deserialized, spills to disk |
| `MEMORY_ONLY_SER`     | ‚úî Yes         | Serialized, saves memory     |
| `MEMORY_AND_DISK_SER` | ‚úî Yes         | Serialized + disk            |
| `DISK_ONLY`           | ‚úî Yes         | Only stored on disk          |

ü§î Why Serialized vs Deserialized Matters?
‚úî Deserialized data
Faster to read
Fast transformations
Uses MORE memory

‚úî Serialized data
Saves memory
Slower to read
Good when memory is limited

‚úî Default when you call .cache()
df.cache()
This uses:
MEMORY_ONLY  (deserialized)







































