ğŸ”¥ Combined Example (Easy to Remember)

Imagine you read a 1 GB file:

Step 1: Reading file â†’ DataFrame partitions

1 GB / 128 MB = ~8 partitions
So df has 8 input partitions.

Step 2: Performing a shuffle

df2 = df.groupBy("city").count()

Now Spark creates 200 shuffle partitions, not 8.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                         1ï¸âƒ£ FILE READING STAGE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 Spark reads RAW FILE(s)
 maxPartitionBytes decides INPUT PARTITION SIZE
 Default = 128 MB (can be increased to 256 MB, etc.)

 Example: 1 GB file â†’ 128 MB chunks â†’ 8 partitions

                   RAW FILE (1 GB)
                   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                           â”‚
      Split into partitions based on maxPartitionBytes
                           v
  +-------------------+  +-------------------+  +-------------------+  ...
  | Input Partition 0 |  | Input Partition 1 |  | Input Partition 2 |
  |     (~128 MB)     |  |     (~128 MB)     |  |     (~128 MB)     |
  +-------------------+  +-------------------+  +-------------------+

 These are the DataFrame partitions CREATED while reading.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                             â–¼
                             â–¼

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                     2ï¸âƒ£ TRANSFORMATION STAGE (NARROW)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 Narrow operations (map, filter, withColumn)
 do NOT change partition count â€” data stays in same partitions.

 Example:
 df = df.filter(col("age") > 25)
 (still same number of input partitions)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                             â–¼
                             â–¼

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                     3ï¸âƒ£ SHUFFLE STAGE (WIDE TRANSFORMATIONS)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 groupBy, join, orderBy, distinct, dropDuplicates â†’ cause SHUFFLE.

 Spark discards the old partitions and CREATES NEW ones.

 Number of shuffle partitions controlled by:
 spark.sql.shuffle.partitions
 Default = 200 (often reduced to 50)

 Example:
 spark.conf.set("spark.sql.shuffle.partitions", 50)
 df2 = df.groupBy("city").count()

 Creates **50 new shuffle partitions**, regardless of input partitions.

                      SHUFFLE OUTPUT PARTITIONS
                   (decided by shuffle.partitions)

     +-------------+ +-------------+ +-------------+ +-------------+ ...
     | Shuffle P0  | | Shuffle P1  | | Shuffle P2  | | Shuffle P3  |
     | (Randomly   | | (Mixed &    | | redistributed| | data)       |
     | redistributed)| (data)      |               |               |
     +-------------+ +-------------+ +-------------+ +-------------+

 These are NEW DataFrame partitions created after shuffle.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                             â–¼
                             â–¼

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                        4ï¸âƒ£ OUTPUT STAGE (WRITING FILES)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 When writing:

 Number of **output files = number of CURRENT DataFrame partitions**

 If DF has 50 partitions â†’ writes 50 files automatically.

 Example:
 df2.write.parquet("path")

                     OUTPUT FILES GENERATED
          part-00000.snappy.parquet
          part-00001.snappy.parquet
          ...
          part-00049.snappy.parquet   (50 files)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                             â–¼
                             â–¼

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                   5ï¸âƒ£ OPTIONAL: MANUALLY CHANGE PARTITIONS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 You can override partition count BEFORE writing:

 repartition(n) â†’ Creates NEW partitions (shuffle)
 coalesce(n)    â†’ Reduces partitions (no shuffle)

 Example:
 df2.repartition(5)
 df2.write.parquet("small_output")

                       NEW 5 OUTPUT PARTITIONS
                +-------------+ +-------------+ +-------------+ 
                | Out Part 0  | | Out Part 1  | | Out Part 2  |
                +-------------+ +-------------+ +-------------+
                ...
                (5 output files)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ¯ **END RESULT: Full Partition Lifecycle**

1ï¸âƒ£ **Input partitions** â†’ created while reading  
2ï¸âƒ£ **Narrow ops** â†’ same partitions  
3ï¸âƒ£ **Shuffle partitions** â†’ new partitions created  
4ï¸âƒ£ **Output partitions** â†’ same as current DF partitions  
5ï¸âƒ£ **Optional repartition/coalesce** â†’ manually override  

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


************************************************************************************ REPARTITION *********************************************************************************
Examples:
1. n partitions
df = df.repartition(10)
df.write.format("parquet").option("mode","overwrite").save("/output/folder")

print(df.rdd.getNumPartitions())
#output
10      #it will create 10 output partition files.

2. repartition by a column
df = df.repartition(col("country"))
df.write.format("parquet").option("mode","overwrite").save("/output/folder")
    #it will club values having same country name and create partition for each country.
    #Number of partitions = Spark default shuffle partitions (200)
df.rdd.getNumPartitions()
#output
200

3. repartition to n partitions using column
df = df.repartition(3,"country")              
df.write.format("parquet").option("mode","overwrite").save("/output/folder")
  #it will create OVERALL 3 output partition files based on hash function applied on country column which will have all the countries data in those 3 files. 
  #NOTE: it WON'T create 3 partition output files for EACH DATE separately since repartition is done in memory and not during write operation.
â— Even though repartition used column "country",
ğŸ‘‰ data is not stored in subfolders
ğŸ‘‰ because you did NOT use partitionBy("country").

4. Using repartition() + partitionBy()
df = df.repartition(3,"country")              
df.write.format("parquet").partitionBy("country").option("mode","overwrite").save("/output/folder")
#output:
/path/output/date=2023-01-01/part-00000.snappy.parquet
/path/output/date=2023-01-02/part-00000.snappy.parquet
/path/output/date=2023-01-03/part-00000.snappy.parquet
...
â— BUT NUMBER OF FILES PER FOLDER â‰  3
Why?
âœ”ï¸ partitionBy("date") overrides Spark partitions
so repartition is done in memory and partitionBy will overwrite repartition and it will create all the partitions.










